from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
import csv
import xml.etree.ElementTree as ET
import re
import os
import logging

from src.config.config_manager import config_manager
from src.domain.entities.test_config import TestConfig
from src.domain.entities.test_result import TestResult
from src.infrastructure.cross_cutting.analysis.test_analyzer import TestAnalyzer
from src.infrastructure.external.testing_tools.jmeter.jmeter_executor import JMeterExecutor
from src.infrastructure.external.testing_tools.jmeter.jmx_handler import JMXHandler
from src.infrastructure.cross_cutting.logging.test_logger import TestLogger
from src.infrastructure.external.file_system.report_generator import ReportGenerator

# åˆ›å»ºloggerå®ä¾‹
logger = logging.getLogger(__name__)

class PerformanceTestService:
    """æ€§èƒ½æµ‹è¯•æœåŠ¡"""
    
    def __init__(self, jmeter_executor: JMeterExecutor, report_generator: ReportGenerator):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•æœåŠ¡
        
        Args:
            jmeter_executor: JMeteræ‰§è¡Œå™¨
            report_generator: æŠ¥å‘Šç”Ÿæˆå™¨
        """
        self.jmeter_executor = jmeter_executor
        self.report_generator = report_generator
        
    def run_tests(self, config: TestConfig) -> List[TestResult]:
        """
        è¿è¡Œæ€§èƒ½æµ‹è¯•
        
        Args:
            config: æµ‹è¯•é…ç½®
            
        Returns:
            List[TestResult]: æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        logger = TestLogger(config.test_name)
        logger.info(f"å¼€å§‹æ‰§è¡Œæµ‹è¯•: {config.test_name}")
        logger.info(f"æµ‹è¯•é…ç½®: è¿­ä»£æ¬¡æ•°={config.iterations}")
        
        # è·å–æ‰€æœ‰çº¿ç¨‹æ•°å’Œå¾ªç¯æ¬¡æ•°ç»„åˆ
        thread_counts = config_manager.get_jmeter_config()['test']['thread_counts']
        loop_counts = config_manager.get_jmeter_config()['test']['loop_counts']
        results = []
        analysis_results = []
        # åŒé‡å¾ªç¯éå†æ‰€æœ‰ç»„åˆ
        for thread_count in thread_counts:
            for loop_count in loop_counts:
                logger.info(f"å¼€å§‹æ‰§è¡Œ çº¿ç¨‹æ•°={thread_count} å¾ªç¯æ¬¡æ•°={loop_count} çš„æµ‹è¯•")
                try:
                    # åˆ›å»ºJMXå¤„ç†å™¨å¹¶æ›´æ–°é…ç½®ï¼ˆä¸ä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼‰
                    jmx_handler = JMXHandler(config.jmx_path)
                    # æ›´æ–°JMXæ–‡ä»¶é…ç½®
                    jmx_handler.update_thread_group(
                        iterations=loop_count,
                        thread_count=thread_count,
                        ramp_time=config_manager.get_jmeter_config()['test']['ramp_up_time']
                    )
                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    temp_jmx_path = str(Path(config.output_dir) / f"temp_{config.test_name}_{thread_count}_{loop_count}.jmx")
                    jmx_handler.save(temp_jmx_path)
                    
                    # ä½¿ç”¨ä¸´æ—¶JMXæ–‡ä»¶æ‰§è¡Œæµ‹è¯•
                    result = self.jmeter_executor.execute_test(
                        jmx_path=temp_jmx_path,
                        iterations=loop_count,
                        output_dir=config.output_dir,
                        test_name=config.test_name,
                        thread_count=thread_count
                    )
                    results.append(result)
                    # åˆ†ææµ‹è¯•ç»“æœï¼Œä¼ é€’çº¿ç¨‹æ•°å‚æ•°
                    analyzer = TestAnalyzer(config.output_dir)
                    analysis_result = analyzer.analyze_test_results(config.test_name, loop_count, thread_count)
                    analysis_results.append(analysis_result)
                    logger.info(f"å®Œæˆ çº¿ç¨‹æ•°={thread_count} å¾ªç¯æ¬¡æ•°={loop_count} çš„æµ‹è¯•")
                    logger.info(f"æ‰§è¡Œæ—¶é•¿: {result.duration:.2f}ç§’")
                    logger.info(f"æŠ¥å‘Šè·¯å¾„: {result.report_path}")
                    logger.info(f"æˆåŠŸç‡: {analysis_result['success_rate']:.2f}%")
                    logger.info(f"å¹³å‡å“åº”æ—¶é—´: {analysis_result['avg_response_time']:.2f}ms")
                except Exception as e:
                    logger.error(f"æ‰§è¡Œ çº¿ç¨‹æ•°={thread_count} å¾ªç¯æ¬¡æ•°={loop_count} çš„æµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    raise
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        try:
            report_path = self.report_generator.generate_summary_report(results)
            logger.info(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š: {report_path}")
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis_report_path = str(Path(config.output_dir) / f"analysis_{config.test_name}.csv")
            analyzer = TestAnalyzer(config.output_dir)
            analyzer.generate_analysis_report(analysis_results, analysis_report_path)
            logger.info(f"ç”Ÿæˆåˆ†ææŠ¥å‘Š: {analysis_report_path}")
            # ç”Ÿæˆé›†æˆæŠ¥å‘Šï¼ˆæµ‹è¯•åç§°+æ—¶é—´æˆ³ï¼‰
            timestamp = results[0].start_time.strftime("%Y%m%d_%H%M%S") if results else datetime.now().strftime("%Y%m%d_%H%M%S")
            integrated_report_path = self.report_generator.generate_integrated_report(
                analysis_csv_path=analysis_report_path,
                summary_csv_path=report_path,
                test_name=config.test_name,
                timestamp=timestamp
            )
            logger.info(f"ç”Ÿæˆé›†æˆæŠ¥å‘Š: {integrated_report_path}")
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
        logger.info("æµ‹è¯•æ‰§è¡Œå®Œæˆ")
        return results 

    def process_register_test_jmx(self, jmx_path: str, thread_counts: list, loop_counts: list, 
                                test_name: str, output_dir: str, device_data_manager) -> str:
        """
        å¤„ç†æ³¨å†Œæµ‹è¯•çš„JMXæ–‡ä»¶ç‰¹æ®Šé€»è¾‘ - åº”ç”¨å±‚ä¸šåŠ¡é€»è¾‘
        
        Args:
            jmx_path: åŸå§‹JMXæ–‡ä»¶è·¯å¾„
            thread_counts: çº¿ç¨‹æ•°åˆ—è¡¨
            loop_counts: å¾ªç¯æ¬¡æ•°åˆ—è¡¨
            test_name: æµ‹è¯•åç§°
            output_dir: è¾“å‡ºç›®å½•
            device_data_manager: è®¾å¤‡æ•°æ®ç®¡ç†å™¨
            
        Returns:
            str: å¤„ç†åçš„JMXæ–‡ä»¶è·¯å¾„
        """
        # è®¡ç®—æœ¬æ¬¡éœ€è¦çš„è®¾å¤‡æ•°é‡ï¼ˆçº¿ç¨‹æ•°*æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼‰
        total_devices_needed = sum(thread_counts) * max(loop_counts)
        
        # è·å–å¯ç”¨è®¾å¤‡æ•°æ®
        device_csv_file = device_data_manager.get_available_devices(total_devices_needed)
        
        # === æ·»åŠ è°ƒè¯•è¾“å‡ºï¼šæ‰“å°CSVæ–‡ä»¶ä¸­çš„å‚æ•°å€¼ ===
        print(f"\nğŸ” [DEBUG] æœ¬æ¬¡æµ‹è¯•å°†ä½¿ç”¨çš„è®¾å¤‡å‚æ•° (CSVæ–‡ä»¶: {device_csv_file}):")
        print("=" * 80)
        try:
            with open(device_csv_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) > 1:  # ç¡®ä¿æœ‰æ•°æ®è¡Œ
                    print(f"ğŸ“‹ CSVæ–‡ä»¶å¤´éƒ¨: {lines[0].strip()}")
                    print("-" * 80)
                    print("ğŸ“Š è®¾å¤‡å‚æ•°åˆ—è¡¨:")
                    for i, line in enumerate(lines[1:], 1):  # è·³è¿‡æ ‡é¢˜è¡Œ
                        if i <= 10:  # åªæ˜¾ç¤ºå‰10è¡Œ
                            parts = line.strip().split(',')
                            if len(parts) >= 2:
                                mac = parts[0]
                                serial_number = parts[1]
                                print(f"  è®¾å¤‡{i:2d}: mac={mac:<20} | deviceSerialNumber={serial_number}")
                        elif i == 11:
                            print(f"  ... è¿˜æœ‰ {len(lines)-11} ä¸ªè®¾å¤‡")
                            break
                    print("-" * 80)
                    print(f"âœ… æ€»å…± {len(lines)-1} ä¸ªè®¾å¤‡å‚æ•°å·²å‡†å¤‡å°±ç»ª")
                else:
                    print("âŒ CSVæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        print("=" * 80)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¾›JMeterè¿›ç¨‹è¯»å–
        os.environ['device_csv_file'] = device_csv_file
        
        # === è¿”å›ä¼ å…¥çš„JMXæ–‡ä»¶è·¯å¾„ï¼Œä¸å†å¼ºåˆ¶ä½¿ç”¨åŸå§‹è·¯å¾„ ===
        print(f"ğŸ¯ [ä½¿ç”¨é…ç½®] JMeterå°†æ‰§è¡ŒJMXæ–‡ä»¶: {jmx_path}")
        return jmx_path

    def update_test_configuration(self, thread_counts: list, loop_counts: list):
        """
        æ›´æ–°æµ‹è¯•é…ç½® - åº”ç”¨å±‚é…ç½®ç®¡ç†
        
        Args:
            thread_counts: çº¿ç¨‹æ•°åˆ—è¡¨
            loop_counts: å¾ªç¯æ¬¡æ•°åˆ—è¡¨
        """
        jmeter_config = config_manager.get_jmeter_config()
        jmeter_config['test']['thread_counts'] = thread_counts
        jmeter_config['test']['loop_counts'] = loop_counts

    def generate_comprehensive_reports(self, results: List[TestResult], test_name: str, 
                                     output_dir: str, generate_excel: bool = False) -> Dict[str, str]:
        """
        ç”Ÿæˆç»¼åˆæŠ¥å‘Š - åº”ç”¨å±‚æŠ¥å‘Šåè°ƒ
        
        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            test_name: æµ‹è¯•åç§°
            output_dir: è¾“å‡ºç›®å½•
            generate_excel: æ˜¯å¦ç”ŸæˆExcelæŠ¥å‘Š
            
        Returns:
            Dict[str, str]: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        from src.application.services.report_service import ReportService
        
        report_service = ReportService(output_dir)
        report_paths = {}
        
        # ç”ŸæˆExcelå¤šsheetæŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if generate_excel:
            excel_report_path = report_service.generate_multi_interface_excel_report(results)
            report_paths['excel'] = excel_report_path
        
        # ç”Ÿæˆæ ‡å‡†CSVæŠ¥å‘Š
        external_report_csv = str(Path(output_dir) / f"{test_name}_å¯¹å¤–æ•´åˆç‰ˆ.csv")
        internal_report_csv = str(Path(output_dir) / f"{test_name}_å†…éƒ¨åˆ†æç‰ˆ.csv")
        param_doc_csv = str(Path(output_dir) / f"{test_name}_å‚æ•°è¯´æ˜è¡¨.csv")
        
        report_service.generate_external_report_from_results(results, external_report_csv)
        report_service.generate_internal_report_from_results(results, internal_report_csv, param_doc_csv)
        
        report_paths.update({
            'external': external_report_csv,
            'internal': internal_report_csv,
            'param_doc': param_doc_csv
        })
        
        return report_paths 

    def execute_comprehensive_test_suite(self, test_configs: list, test_name: str, 
                                       output_dir: str, generate_excel: bool = False,
                                       enable_verification: bool = True) -> dict:
        """
        æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ - åº”ç”¨å±‚æµ‹è¯•åè°ƒ
        
        èŒè´£ï¼šåè°ƒæ•´ä¸ªæµ‹è¯•æµç¨‹ï¼ŒåŒ…æ‹¬æµ‹è¯•æ‰§è¡Œã€æ³¨å†ŒéªŒè¯ã€æŠ¥å‘Šç”Ÿæˆç­‰
        
        Args:
            test_configs: æµ‹è¯•é…ç½®åˆ—è¡¨
            test_name: æµ‹è¯•åç§°
            output_dir: è¾“å‡ºç›®å½•
            generate_excel: æ˜¯å¦ç”ŸæˆExcelæŠ¥å‘Š
            enable_verification: æ˜¯å¦å¯ç”¨æ³¨å†ŒéªŒè¯
            
        Returns:
            dict: åŒ…å«æµ‹è¯•ç»“æœã€éªŒè¯ç»“æœã€æŠ¥å‘Šè·¯å¾„çš„å®Œæ•´ç»“æœ
        """
        all_results = []
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for config in test_configs:
            results = self.run_tests(config)
            all_results.extend(results)
        
        # å¤„ç†æ³¨å†ŒéªŒè¯
        verification_result = None
        if enable_verification and any('register' in config.test_name for config in test_configs):
            verification_result = self._handle_register_verification(all_results, output_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_paths = self.generate_comprehensive_reports(all_results, test_name, output_dir, generate_excel)
        
        return {
            'test_results': all_results,
            'verification_result': verification_result,
            'report_paths': report_paths,
            'output_dir': output_dir
        }

    def _handle_register_verification(self, all_results: list, output_dir: str) -> dict:
        """
        å¤„ç†æ³¨å†ŒéªŒè¯ - åº”ç”¨å±‚éªŒè¯åè°ƒ
        
        Args:
            all_results: æ‰€æœ‰æµ‹è¯•ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        try:
            from src.application.services.register_verification_service import RegisterVerificationService
            from src.config.config_manager import config_manager
            
            database_config = config_manager.get_database_config()
            mysql_config = database_config.get('mysql', {})
            db_config = {
                'host': mysql_config.get('host', 'localhost'),
                'port': mysql_config.get('port', 3306),
                'user': mysql_config.get('username', 'root'),
                'password': mysql_config.get('password', ''),
                'database': mysql_config.get('database', '')
            }
            
            verification_service = RegisterVerificationService(db_config)
            verification_report = verification_service.verify_registration_results(
                all_results, os.path.basename(output_dir)
            )
            
            # ä¿å­˜éªŒè¯æŠ¥å‘Š
            verification_service.save_verification_report(verification_report, output_dir)
            
            return {
                'success': True,
                'report': verification_report,
                'report_path': f"{output_dir}/verification_report_{os.path.basename(output_dir)}.json"
            }
            
        except Exception as e:
            logger.error(f"æ³¨å†ŒéªŒè¯å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def create_test_configs(self, test_types: list, thread_counts: list, loop_counts: list,
                          test_name: str, jmeter_bin: str, output_dir: str, base_jmx_dir: str) -> list:
        """
        åˆ›å»ºæµ‹è¯•é…ç½®åˆ—è¡¨ - åº”ç”¨å±‚é…ç½®ç®¡ç†
        
        Args:
            test_types: æµ‹è¯•ç±»å‹åˆ—è¡¨
            thread_counts: çº¿ç¨‹æ•°åˆ—è¡¨
            loop_counts: å¾ªç¯æ¬¡æ•°åˆ—è¡¨
            test_name: æµ‹è¯•åç§°
            jmeter_bin: JMeterå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            base_jmx_dir: JMXæ–‡ä»¶åŸºç¡€ç›®å½•
            
        Returns:
            list: æµ‹è¯•é…ç½®åˆ—è¡¨
        """
        configs = []
        
        for test_type in test_types:
            jmx_path = os.path.join(base_jmx_dir, f"{test_type}_test.jmx")
            
            # å¤„ç†registeræ¥å£çš„ç‰¹æ®Šé€»è¾‘
            if test_type == 'register':
                from src.application.services.device_data_manager import DeviceDataManager
                device_data_manager = DeviceDataManager()
                processed_jmx_path = self.process_register_test_jmx(
                    jmx_path, thread_counts, loop_counts, test_name, 
                    output_dir, device_data_manager
                )
                jmx_path = processed_jmx_path
            
            config = TestConfig(
                test_name=f"{test_name}_{test_type}",
                iterations=loop_counts,
                jmx_path=jmx_path,
                jmeter_bin_path=jmeter_bin,
                output_dir=output_dir
            )
            configs.append(config)
        
        return configs 
