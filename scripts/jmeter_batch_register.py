#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
JMETERæ‰¹é‡æ³¨å†Œè„šæœ¬
- å®Œå…¨æ›¿ä»£å›¾å½¢ç•Œé¢æ“ä½œ
- æ”¯æŒä¸åŒçº¿ç¨‹æ•°çš„æ‰¹é‡æ³¨å†Œ
- è‡ªåŠ¨éªŒè¯æ³¨å†Œæ•°é‡
- ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""
import os
import sys
import subprocess
import time
import json
import csv
from datetime import datetime
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.insert(0, project_root)

from src.application.services.device_query_service import DeviceQueryService

class JMeterBatchRegister:
    """JMETERæ‰¹é‡æ³¨å†Œç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.jmeter_bin = os.path.join(project_root, "src", "tools", "jmeter", "bin", "jmeter.bat")
        self.jmx_file = os.path.join(project_root, "src", "tools", "jmeter", "bin", "register_test_python_format.jmx")
        self.csv_file = os.path.join(project_root, "src", "tools", "jmeter", "bin", "new_devices_100000.csv")
        self.results_dir = os.path.join(project_root, "src", "tools", "jmeter", "results")
        
        # æ•°æ®åº“é…ç½®
        self.db_config = {
            'host': '192.168.24.45',
            'port': 3307,
            'user': 'root',
            'password': 'At6mj*1ygb2',
            'database': 'yangguan'
        }
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.results_dir, exist_ok=True)
        
    def check_prerequisites(self):
        """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
        print("ğŸ” æ£€æŸ¥å‰ç½®æ¡ä»¶...")
        
        # æ£€æŸ¥JMETERå¯æ‰§è¡Œæ–‡ä»¶
        if not os.path.exists(self.jmeter_bin):
            raise FileNotFoundError(f"JMETERå¯æ‰§è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {self.jmeter_bin}")
        
        # æ£€æŸ¥JMXæ–‡ä»¶
        if not os.path.exists(self.jmx_file):
            raise FileNotFoundError(f"JMXæ–‡ä»¶ä¸å­˜åœ¨: {self.jmx_file}")
        
        # æ£€æŸ¥CSVæ–‡ä»¶
        if not os.path.exists(self.csv_file):
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_file}")
        
        print("âœ… å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡")
        
    def get_device_count_before(self):
        """è·å–æ³¨å†Œå‰çš„è®¾å¤‡æ•°é‡"""
        print("ğŸ“Š è·å–æ³¨å†Œå‰çš„è®¾å¤‡æ•°é‡...")
        try:
            service = DeviceQueryService(self.db_config)
            count = service.get_device_count('biz_device')
            service.close()
            print(f"ğŸ“ˆ æ³¨å†Œå‰è®¾å¤‡æ€»æ•°: {count}")
            return count
        except Exception as e:
            print(f"âŒ è·å–è®¾å¤‡æ•°é‡å¤±è´¥: {e}")
            return 0
    
    def modify_jmx_threads(self, thread_count, loops=1):
        """ä¿®æ”¹JMXæ–‡ä»¶ä¸­çš„çº¿ç¨‹æ•°å’Œå¾ªç¯æ¬¡æ•°"""
        print(f"ğŸ”§ ä¿®æ”¹JMXé…ç½®: çº¿ç¨‹æ•°={thread_count}, å¾ªç¯æ¬¡æ•°={loops}")
        
        # è¯»å–JMXæ–‡ä»¶
        with open(self.jmx_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä¿®æ”¹çº¿ç¨‹æ•°
        import re
        content = re.sub(
            r'<intProp name="ThreadGroup\.num_threads">\d+</intProp>',
            f'<intProp name="ThreadGroup.num_threads">{thread_count}</intProp>',
            content
        )
        
        # ä¿®æ”¹å¾ªç¯æ¬¡æ•°
        content = re.sub(
            r'<stringProp name="LoopController\.loops">\d+</stringProp>',
            f'<stringProp name="LoopController.loops">{loops}</stringProp>',
            content
        )
        
        # ä¿å­˜ä¿®æ”¹åçš„JMXæ–‡ä»¶
        modified_jmx = os.path.join(self.results_dir, f"register_test_{thread_count}threads_{loops}loops.jmx")
        with open(modified_jmx, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ ä¿®æ”¹åçš„JMXæ–‡ä»¶å·²ä¿å­˜: {modified_jmx}")
        return modified_jmx
    
    def run_jmeter_test(self, jmx_path, test_name):
        """è¿è¡ŒJMETERæµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹æ‰§è¡ŒJMETERæµ‹è¯•: {test_name}")
        
        # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        jtl_file = os.path.join(self.results_dir, f"{test_name}_{timestamp}.jtl")
        log_file = os.path.join(self.results_dir, f"{test_name}_{timestamp}.log")
        report_dir = os.path.join(self.results_dir, f"{test_name}_{timestamp}_report")
        
        # æ„å»ºJMETERå‘½ä»¤
        cmd = [
            self.jmeter_bin,
            "-n",  # éGUIæ¨¡å¼
            "-t", jmx_path,  # æµ‹è¯•è®¡åˆ’æ–‡ä»¶
            "-l", jtl_file,  # ç»“æœæ–‡ä»¶
            "-j", log_file,  # æ—¥å¿—æ–‡ä»¶
            "-e",  # ç”ŸæˆHTMLæŠ¥å‘Š
            "-o", report_dir  # æŠ¥å‘Šè¾“å‡ºç›®å½•
        ]
        
        print(f"ğŸ“‹ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        try:
            # æ‰§è¡ŒJMETERå‘½ä»¤
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                cwd=os.path.dirname(self.jmeter_bin)
            )
            
            # å®æ—¶è¾“å‡ºè¿›åº¦
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(f"   {output.strip()}")
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if return_code == 0:
                print(f"âœ… JMETERæµ‹è¯•å®Œæˆ: {test_name}")
                print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
                print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {jtl_file}")
                print(f"ğŸ“ˆ æŠ¥å‘Šç›®å½•: {report_dir}")
                return True, jtl_file, report_dir, duration
            else:
                error_output = process.stderr.read()
                print(f"âŒ JMETERæµ‹è¯•å¤±è´¥: {test_name}")
                print(f"é”™è¯¯ä¿¡æ¯: {error_output}")
                return False, None, None, duration
                
        except Exception as e:
            print(f"âŒ æ‰§è¡ŒJMETERå‘½ä»¤å¤±è´¥: {e}")
            return False, None, None, 0
    
    def get_device_count_after(self):
        """è·å–æ³¨å†Œåçš„è®¾å¤‡æ•°é‡"""
        print("ğŸ“Š è·å–æ³¨å†Œåçš„è®¾å¤‡æ•°é‡...")
        try:
            service = DeviceQueryService(self.db_config)
            count = service.get_device_count('biz_device')
            service.close()
            print(f"ğŸ“ˆ æ³¨å†Œåè®¾å¤‡æ€»æ•°: {count}")
            return count
        except Exception as e:
            print(f"âŒ è·å–è®¾å¤‡æ•°é‡å¤±è´¥: {e}")
            return 0
    
    def analyze_jtl_file(self, jtl_file):
        """åˆ†æJTLæ–‡ä»¶ï¼Œç»Ÿè®¡æµ‹è¯•ç»“æœ"""
        print(f"ğŸ“Š åˆ†ææµ‹è¯•ç»“æœ: {jtl_file}")
        
        if not os.path.exists(jtl_file):
            print(f"âŒ JTLæ–‡ä»¶ä¸å­˜åœ¨: {jtl_file}")
            return {}
        
        stats = {
            'total_requests': 0,
            'success_count': 0,
            'fail_count': 0,
            'avg_response_time': 0,
            'min_response_time': float('inf'),
            'max_response_time': 0,
            'response_times': []
        }
        
        try:
            with open(jtl_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    stats['total_requests'] += 1
                    
                    # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
                    if row.get('success', '').lower() == 'true':
                        stats['success_count'] += 1
                    else:
                        stats['fail_count'] += 1
                    
                    # ç»Ÿè®¡å“åº”æ—¶é—´
                    try:
                        response_time = float(row.get('elapsed', 0))
                        stats['response_times'].append(response_time)
                        stats['min_response_time'] = min(stats['min_response_time'], response_time)
                        stats['max_response_time'] = max(stats['max_response_time'], response_time)
                    except (ValueError, TypeError):
                        pass
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            if stats['response_times']:
                stats['avg_response_time'] = sum(stats['response_times']) / len(stats['response_times'])
                stats['min_response_time'] = min(stats['response_times'])
                stats['max_response_time'] = max(stats['response_times'])
            
            print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            print(f"   æˆåŠŸæ•°: {stats['success_count']}")
            print(f"   å¤±è´¥æ•°: {stats['fail_count']}")
            print(f"   æˆåŠŸç‡: {stats['success_count']/stats['total_requests']*100:.2f}%" if stats['total_requests'] > 0 else "   æˆåŠŸç‡: 0%")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ms")
            print(f"   æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.2f}ms")
            print(f"   æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.2f}ms")
            
            return stats
            
        except Exception as e:
            print(f"âŒ åˆ†æJTLæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def run_batch_test(self, thread_configs):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹æ‰¹é‡æ³¨å†Œæµ‹è¯•")
        print("=" * 60)
        
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        self.check_prerequisites()
        
        # è·å–æ³¨å†Œå‰çš„è®¾å¤‡æ•°é‡
        count_before = self.get_device_count_before()
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        test_results = []
        
        # æ‰§è¡Œæ¯ä¸ªé…ç½®çš„æµ‹è¯•
        for i, config in enumerate(thread_configs):
            thread_count = config['threads']
            loops = config.get('loops', 1)
            
            print(f"\n{'='*20} æµ‹è¯•é…ç½® {i+1}/{len(thread_configs)} {'='*20}")
            print(f"ğŸ§µ çº¿ç¨‹æ•°: {thread_count}")
            print(f"ğŸ”„ å¾ªç¯æ¬¡æ•°: {loops}")
            print(f"ğŸ“Š é¢„æœŸæ€»è¯·æ±‚æ•°: {thread_count * loops}")
            
            # ä¿®æ”¹JMXæ–‡ä»¶
            modified_jmx = self.modify_jmx_threads(thread_count, loops)
            
            # è¿è¡Œæµ‹è¯•
            test_name = f"register_{thread_count}threads_{loops}loops"
            success, jtl_file, report_dir, duration = self.run_jmeter_test(modified_jmx, test_name)
            
            if success:
                # åˆ†ææµ‹è¯•ç»“æœ
                stats = self.analyze_jtl_file(jtl_file)
                
                # è·å–æ³¨å†Œåçš„è®¾å¤‡æ•°é‡
                count_after = self.get_device_count_after()
                registered_count = count_after - count_before
                
                # è®°å½•æµ‹è¯•ç»“æœ
                result = {
                    'test_name': test_name,
                    'thread_count': thread_count,
                    'loops': loops,
                    'expected_requests': thread_count * loops,
                    'actual_requests': stats.get('total_requests', 0),
                    'success_count': stats.get('success_count', 0),
                    'fail_count': stats.get('fail_count', 0),
                    'success_rate': stats.get('success_count', 0) / stats.get('total_requests', 1) * 100 if stats.get('total_requests', 0) > 0 else 0,
                    'avg_response_time': stats.get('avg_response_time', 0),
                    'min_response_time': stats.get('min_response_time', 0),
                    'max_response_time': stats.get('max_response_time', 0),
                    'duration': duration,
                    'count_before': count_before,
                    'count_after': count_after,
                    'registered_count': registered_count,
                    'jtl_file': jtl_file,
                    'report_dir': report_dir,
                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                test_results.append(result)
                
                print(f"âœ… æµ‹è¯•å®Œæˆ: {test_name}")
                print(f"ğŸ“Š å®é™…æ³¨å†Œæ•°é‡: {registered_count}")
                print(f"ğŸ“ˆ æ³¨å†ŒæˆåŠŸç‡: {registered_count/(thread_count*loops)*100:.2f}%" if thread_count*loops > 0 else "ğŸ“ˆ æ³¨å†ŒæˆåŠŸç‡: 0%")
                
                # æ›´æ–°åŸºå‡†æ•°é‡
                count_before = count_after
                
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {test_name}")
            
            # æµ‹è¯•é—´éš”
            if i < len(thread_configs) - 1:
                print(f"\nâ³ ç­‰å¾…30ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(30)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_summary_report(test_results)
        
        return test_results
    
    def generate_summary_report(self, test_results):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        csv_file = os.path.join(self.results_dir, f"batch_register_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = [
                'æµ‹è¯•åç§°', 'çº¿ç¨‹æ•°', 'å¾ªç¯æ¬¡æ•°', 'é¢„æœŸè¯·æ±‚æ•°', 'å®é™…è¯·æ±‚æ•°',
                'æˆåŠŸæ•°', 'å¤±è´¥æ•°', 'æˆåŠŸç‡(%)', 'å¹³å‡å“åº”æ—¶é—´(ms)',
                'æœ€å°å“åº”æ—¶é—´(ms)', 'æœ€å¤§å“åº”æ—¶é—´(ms)', 'æ‰§è¡Œæ—¶é—´(ç§’)',
                'æ³¨å†Œå‰è®¾å¤‡æ•°', 'æ³¨å†Œåè®¾å¤‡æ•°', 'å®é™…æ³¨å†Œæ•°', 'æ³¨å†ŒæˆåŠŸç‡(%)',
                'JTLæ–‡ä»¶', 'æŠ¥å‘Šç›®å½•', 'æµ‹è¯•æ—¶é—´'
            ]
            
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in test_results:
                writer.writerow({
                    'æµ‹è¯•åç§°': result['test_name'],
                    'çº¿ç¨‹æ•°': result['thread_count'],
                    'å¾ªç¯æ¬¡æ•°': result['loops'],
                    'é¢„æœŸè¯·æ±‚æ•°': result['expected_requests'],
                    'å®é™…è¯·æ±‚æ•°': result['actual_requests'],
                    'æˆåŠŸæ•°': result['success_count'],
                    'å¤±è´¥æ•°': result['fail_count'],
                    'æˆåŠŸç‡(%)': f"{result['success_rate']:.2f}",
                    'å¹³å‡å“åº”æ—¶é—´(ms)': f"{result['avg_response_time']:.2f}",
                    'æœ€å°å“åº”æ—¶é—´(ms)': f"{result['min_response_time']:.2f}",
                    'æœ€å¤§å“åº”æ—¶é—´(ms)': f"{result['max_response_time']:.2f}",
                    'æ‰§è¡Œæ—¶é—´(ç§’)': f"{result['duration']:.2f}",
                    'æ³¨å†Œå‰è®¾å¤‡æ•°': result['count_before'],
                    'æ³¨å†Œåè®¾å¤‡æ•°': result['count_after'],
                    'å®é™…æ³¨å†Œæ•°': result['registered_count'],
                    'æ³¨å†ŒæˆåŠŸç‡(%)': f"{result['registered_count']/result['expected_requests']*100:.2f}" if result['expected_requests'] > 0 else "0.00",
                    'JTLæ–‡ä»¶': result['jtl_file'],
                    'æŠ¥å‘Šç›®å½•': result['report_dir'],
                    'æµ‹è¯•æ—¶é—´': result['timestamp']
                })
        
        print(f"ğŸ“Š CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {csv_file}")
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        json_file = os.path.join(self.results_dir, f"batch_register_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {json_file}")
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ¯ æ‰¹é‡æ³¨å†Œæµ‹è¯•æ€»ç»“")
        print("="*60)
        
        total_expected = sum(r['expected_requests'] for r in test_results)
        total_actual = sum(r['actual_requests'] for r in test_results)
        total_success = sum(r['success_count'] for r in test_results)
        total_registered = sum(r['registered_count'] for r in test_results)
        
        print(f"ğŸ“Š æ€»é¢„æœŸè¯·æ±‚æ•°: {total_expected}")
        print(f"ğŸ“Š æ€»å®é™…è¯·æ±‚æ•°: {total_actual}")
        print(f"ğŸ“Š æ€»æˆåŠŸè¯·æ±‚æ•°: {total_success}")
        print(f"ğŸ“Š æ€»æ³¨å†Œè®¾å¤‡æ•°: {total_registered}")
        print(f"ğŸ“ˆ æ•´ä½“æˆåŠŸç‡: {total_success/total_actual*100:.2f}%" if total_actual > 0 else "ğŸ“ˆ æ•´ä½“æˆåŠŸç‡: 0%")
        print(f"ğŸ“ˆ æ•´ä½“æ³¨å†Œç‡: {total_registered/total_expected*100:.2f}%" if total_expected > 0 else "ğŸ“ˆ æ•´ä½“æ³¨å†Œç‡: 0%")
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {self.results_dir}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='JMETERæ‰¹é‡æ³¨å†Œè„šæœ¬')
    parser.add_argument('--threads', nargs='+', type=int, default=[100, 500, 1000],
                       help='çº¿ç¨‹æ•°åˆ—è¡¨ï¼Œä¾‹å¦‚: 100 500 1000')
    parser.add_argument('--loops', nargs='+', type=int, default=[1],
                       help='å¾ªç¯æ¬¡æ•°åˆ—è¡¨ï¼Œä¾‹å¦‚: 1 5 10')
    parser.add_argument('--single', action='store_true',
                       help='å•æ¬¡æµ‹è¯•æ¨¡å¼ï¼Œåªæ‰§è¡Œä¸€ä¸ªé…ç½®')
    
    args = parser.parse_args()
    
    # æ„å»ºæµ‹è¯•é…ç½®
    if args.single:
        # å•æ¬¡æµ‹è¯•æ¨¡å¼
        thread_configs = [
            {'threads': args.threads[0], 'loops': args.loops[0]}
        ]
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        thread_configs = []
        for thread_count in args.threads:
            for loops in args.loops:
                thread_configs.append({
                    'threads': thread_count,
                    'loops': loops
                })
    
    print("ğŸ¯ JMETERæ‰¹é‡æ³¨å†Œè„šæœ¬")
    print("="*60)
    print(f"ğŸ“‹ æµ‹è¯•é…ç½®æ•°é‡: {len(thread_configs)}")
    for i, config in enumerate(thread_configs):
        print(f"   é…ç½® {i+1}: {config['threads']}çº¿ç¨‹ Ã— {config['loops']}å¾ªç¯")
    print("="*60)
    
    # åˆ›å»ºæ³¨å†Œç®¡ç†å™¨å¹¶æ‰§è¡Œæµ‹è¯•
    register_manager = JMeterBatchRegister()
    results = register_manager.run_batch_test(thread_configs)
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == '__main__':
    main() 