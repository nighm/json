#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ„ç¬¬äºŒé˜¶æ®µï¼šæˆ˜åœºè§„åˆ’è‡ªåŠ¨åŒ–åˆ†æ
åˆ†æé¡¹ç›®æ¶æ„ã€ä¾èµ–å…³ç³»ã€ç¡®å®šé‡æ„ä¼˜å…ˆçº§å’Œé£é™©ç‚¹

ä½œè€…ï¼šAI Assistant
åˆ›å»ºæ—¶é—´ï¼š2025-01-27
"""

import os
import sys
import json
import re
from pathlib import Path
from typing import List, Dict, Set
from datetime import datetime
import logging
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ArchitectureAnalyzer:
    """æ¶æ„åˆ†æå™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.output_dir = project_root / "docs" / "development"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # DDDåˆ†å±‚å®šä¹‰
        self.ddd_layers = {
            'interfaces': ['cli', 'api', 'web', 'main'],
            'application': ['services', 'use_cases', 'application'],
            'domain': ['entities', 'value_objects', 'domain_services', 'repositories'],
            'infrastructure': ['persistence', 'external', 'cross_cutting']
        }

    def analyze_architecture_patterns(self) -> List[Dict]:
        """åˆ†æå½“å‰æ¶æ„æ¨¡å¼"""
        logger.info("å¼€å§‹åˆ†ææ¶æ„æ¨¡å¼...")
        
        patterns = []
        
        # æ£€æŸ¥DDDåˆ†å±‚ç»“æ„
        ddd_compliance = self._check_ddd_compliance()
        patterns.append({
            'pattern': 'DDDåˆ†å±‚æ¶æ„',
            'compliance': ddd_compliance['compliance_rate'],
            'issues': ddd_compliance['issues'],
            'recommendations': ddd_compliance['recommendations']
        })
        
        # æ£€æŸ¥æ¨¡å—åŒ–ç¨‹åº¦
        modularity = self._analyze_modularity()
        patterns.append({
            'pattern': 'æ¨¡å—åŒ–è®¾è®¡',
            'compliance': modularity['modularity_score'],
            'issues': modularity['issues'],
            'recommendations': modularity['recommendations']
        })
        
        return patterns

    def _check_ddd_compliance(self) -> Dict:
        """æ£€æŸ¥DDDåˆ†å±‚æ¶æ„åˆè§„æ€§"""
        src_dir = self.project_root / "src"
        if not src_dir.exists():
            return {
                'compliance_rate': 0.0,
                'issues': ['ç¼ºå°‘srcç›®å½•ï¼Œæœªé‡‡ç”¨DDDåˆ†å±‚æ¶æ„'],
                'recommendations': ['åˆ›å»ºsrcç›®å½•ï¼ŒæŒ‰DDDåˆ†å±‚ç»„ç»‡ä»£ç ']
            }
        
        layer_counts = {}
        issues = []
        recommendations = []
        
        # ç»Ÿè®¡å„å±‚æ–‡ä»¶æ•°é‡
        for layer, subdirs in self.ddd_layers.items():
            layer_path = src_dir / layer
            if layer_path.exists():
                file_count = len(list(layer_path.rglob("*.py")))
                layer_counts[layer] = file_count
            else:
                layer_counts[layer] = 0
                issues.append(f"ç¼ºå°‘{layer}å±‚ç›®å½•")
                recommendations.append(f"åˆ›å»ºsrc/{layer}ç›®å½•")
        
        # è®¡ç®—åˆè§„ç‡
        total_layers = len(self.ddd_layers)
        existing_layers = sum(1 for count in layer_counts.values() if count > 0)
        compliance_rate = existing_layers / total_layers
        
        return {
            'compliance_rate': compliance_rate,
            'layer_counts': layer_counts,
            'issues': issues,
            'recommendations': recommendations
        }

    def _analyze_modularity(self) -> Dict:
        """åˆ†ææ¨¡å—åŒ–ç¨‹åº¦"""
        src_dir = self.project_root / "src"
        if not src_dir.exists():
            return {
                'modularity_score': 0.0,
                'issues': ['ç¼ºå°‘æ¨¡å—åŒ–ç»“æ„'],
                'recommendations': ['å»ºç«‹æ¨¡å—åŒ–ç›®å½•ç»“æ„']
            }
        
        # ç»Ÿè®¡æ¨¡å—æ•°é‡
        modules = []
        for item in src_dir.iterdir():
            if item.is_dir():
                py_files = list(item.rglob("*.py"))
                modules.append({
                    'name': item.name,
                    'file_count': len(py_files),
                    'size': sum(f.stat().st_size for f in py_files if f.is_file())
                })
        
        # è®¡ç®—æ¨¡å—åŒ–è¯„åˆ†
        if not modules:
            return {
                'modularity_score': 0.0,
                'issues': ['srcç›®å½•ä¸‹æ— å­æ¨¡å—'],
                'recommendations': ['åˆ›å»ºåŠŸèƒ½æ¨¡å—ç›®å½•']
            }
        
        modularity_score = min(1.0, len(modules) / 10)
        
        issues = []
        recommendations = []
        
        if modularity_score < 0.5:
            issues.append("æ¨¡å—åŒ–ç¨‹åº¦è¾ƒä½")
            recommendations.append("å¢åŠ æ¨¡å—åˆ’åˆ†ï¼Œå¹³è¡¡æ¨¡å—å¤§å°")
        
        return {
            'modularity_score': modularity_score,
            'modules': modules,
            'issues': issues,
            'recommendations': recommendations
        }

    def analyze_dependencies(self) -> Dict:
        """åˆ†ææ¨¡å—é—´ä¾èµ–å…³ç³»"""
        logger.info("å¼€å§‹åˆ†æä¾èµ–å…³ç³»...")
        
        dependency_graph = defaultdict(set)
        
        # æ‰«ææ‰€æœ‰Pythonæ–‡ä»¶
        python_files = list(self.project_root.rglob("*.py"))
        
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£æå¯¼å…¥è¯­å¥
                imports = self._extract_imports(content)
                module_name = self._get_module_name(py_file)
                
                for imp in imports:
                    dependency_graph[module_name].add(imp)
                    
            except Exception as e:
                logger.warning(f"åˆ†ææ–‡ä»¶å¤±è´¥ {py_file}: {e}")
        
        # è®¡ç®—ä¾èµ–æŒ‡æ ‡
        dependency_metrics = self._calculate_dependency_metrics(dependency_graph)
        
        return {
            'graph': dict(dependency_graph),
            'metrics': dependency_metrics
        }

    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            '__pycache__', 'venv', '.git', 'node_modules',
            'site-packages', 'tests', 'test_'
        ]
        
        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _extract_imports(self, content: str) -> List[str]:
        """æå–å¯¼å…¥è¯­å¥"""
        imports = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯¼å…¥
        import_patterns = [
            r'^import\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s*$',
            r'^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import',
        ]
        
        for line in content.split('\n'):
            line = line.strip()
            for pattern in import_patterns:
                match = re.match(pattern, line)
                if match:
                    imports.append(match.group(1))
                    break
        
        return imports

    def _get_module_name(self, file_path: Path) -> str:
        """è·å–æ¨¡å—åç§°"""
        relative_path = file_path.relative_to(self.project_root)
        return str(relative_path).replace('\\', '.').replace('/', '.').replace('.py', '')

    def _calculate_dependency_metrics(self, dependency_graph: Dict) -> Dict:
        """è®¡ç®—ä¾èµ–æŒ‡æ ‡"""
        metrics = {
            'total_modules': len(dependency_graph),
            'avg_dependencies': 0,
            'max_dependencies': 0,
            'high_coupling_modules': []
        }
        
        if dependency_graph:
            # è®¡ç®—å¹³å‡ä¾èµ–æ•°
            total_deps = sum(len(deps) for deps in dependency_graph.values())
            metrics['avg_dependencies'] = total_deps / len(dependency_graph)
            metrics['max_dependencies'] = max(len(deps) for deps in dependency_graph.values())
            
            # è¯†åˆ«é«˜è€¦åˆæ¨¡å—
            threshold = metrics['avg_dependencies'] * 2
            for module, deps in dependency_graph.items():
                if len(deps) > threshold:
                    metrics['high_coupling_modules'].append({
                        'module': module,
                        'dependencies': len(deps)
                    })
        
        return metrics

    def determine_refactor_priorities(self, patterns: List[Dict], dependencies: Dict) -> List[Dict]:
        """ç¡®å®šé‡æ„ä¼˜å…ˆçº§"""
        logger.info("å¼€å§‹ç¡®å®šé‡æ„ä¼˜å…ˆçº§...")
        
        priorities = []
        
        # åŸºäºæ¶æ„é—®é¢˜ç¡®å®šä¼˜å…ˆçº§
        for pattern in patterns:
            if pattern['compliance'] < 0.5:
                priorities.append({
                    'module': 'architecture',
                    'priority': 'high',
                    'reason': f"{pattern['pattern']}åˆè§„æ€§ä½ ({pattern['compliance']:.1%})",
                    'impact': 'high',
                    'effort': 'high'
                })
        
        # åŸºäºä¾èµ–å…³ç³»ç¡®å®šä¼˜å…ˆçº§
        if 'metrics' in dependencies:
            for module_info in dependencies['metrics']['high_coupling_modules']:
                priorities.append({
                    'module': module_info['module'],
                    'priority': 'high',
                    'reason': f"è€¦åˆåº¦é«˜ ({module_info['dependencies']} ä¸ªä¾èµ–)",
                    'impact': 'high',
                    'effort': 'high'
                })
        
        return priorities

    def identify_risk_points(self, patterns: List[Dict], dependencies: Dict) -> List[Dict]:
        """è¯†åˆ«é‡æ„é£é™©ç‚¹"""
        logger.info("å¼€å§‹è¯†åˆ«é£é™©ç‚¹...")
        
        risks = []
        
        # åŸºäºä¾èµ–å…³ç³»è¯†åˆ«é£é™©
        if 'metrics' in dependencies:
            if dependencies['metrics']['avg_dependencies'] > 5:
                risks.append({
                    'type': 'dependency_risk',
                    'description': 'æ¨¡å—é—´ä¾èµ–è¿‡å¤šï¼Œé‡æ„å¯èƒ½å½±å“èŒƒå›´å¤§',
                    'severity': 'high',
                    'mitigation': 'åˆ†é˜¶æ®µé‡æ„ï¼Œå…ˆè§£è€¦æ ¸å¿ƒæ¨¡å—'
                })
        
        # åŸºäºæ¶æ„é—®é¢˜è¯†åˆ«é£é™©
        for pattern in patterns:
            if pattern['compliance'] < 0.3:
                risks.append({
                    'type': 'architecture_risk',
                    'description': f'{pattern["pattern"]}ä¸¥é‡ä¸ç¬¦åˆè§„èŒƒ',
                    'severity': 'high',
                    'mitigation': 'åˆ¶å®šè¯¦ç»†çš„é‡æ„è®¡åˆ’ï¼Œåˆ†æ­¥éª¤å®æ–½'
                })
        
        return risks

    def generate_planning_report(self) -> str:
        """ç”Ÿæˆè§„åˆ’æŠ¥å‘Š"""
        logger.info("å¼€å§‹ç”Ÿæˆè§„åˆ’æŠ¥å‘Š...")
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        patterns = self.analyze_architecture_patterns()
        dependencies = self.analyze_dependencies()
        priorities = self.determine_refactor_priorities(patterns, dependencies)
        risks = self.identify_risk_points(patterns, dependencies)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = self.output_dir / "phase2_planning_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(self._format_planning_report(patterns, dependencies, priorities, risks))
        
        logger.info(f"è§„åˆ’æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

    def _format_planning_report(self, patterns: List[Dict], dependencies: Dict, priorities: List[Dict], risks: List[Dict]) -> str:
        """æ ¼å¼åŒ–è§„åˆ’æŠ¥å‘Š"""
        report = f"""# é‡æ„ç¬¬äºŒé˜¶æ®µï¼šæˆ˜åœºè§„åˆ’æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## 1. æ¶æ„è®¾è®¡åˆ†æ

### 1.1 æ¶æ„æ¨¡å¼è¯„ä¼°

"""
        
        for pattern in patterns:
            report += f"""#### {pattern['pattern']}
- **åˆè§„ç‡**: {pattern['compliance']:.1%}
- **é—®é¢˜**: {', '.join(pattern['issues']) if pattern['issues'] else 'æ— '}
- **å»ºè®®**: {', '.join(pattern['recommendations']) if pattern['recommendations'] else 'æ— '}

"""
        
        # ä¾èµ–å…³ç³»åˆ†æ
        if 'metrics' in dependencies:
            metrics = dependencies['metrics']
            report += f"""## 2. ä¾èµ–å…³ç³»åˆ†æ

### 2.1 ä¾èµ–æŒ‡æ ‡
- **æ€»æ¨¡å—æ•°**: {metrics['total_modules']}
- **å¹³å‡ä¾èµ–æ•°**: {metrics['avg_dependencies']:.2f}
- **æœ€å¤§ä¾èµ–æ•°**: {metrics['max_dependencies']}

### 2.2 é«˜è€¦åˆæ¨¡å—
"""
            for module in metrics['high_coupling_modules']:
                report += f"- {module['module']}: {module['dependencies']} ä¸ªä¾èµ–\n"
        
        # é‡æ„ä¼˜å…ˆçº§
        report += f"""
## 3. é‡æ„ä¼˜å…ˆçº§

### 3.1 é«˜ä¼˜å…ˆçº§æ¨¡å—
"""
        high_priority = [p for p in priorities if p['priority'] == 'high']
        for priority in high_priority[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            report += f"""- **{priority['module']}**
  - åŸå› : {priority['reason']}
  - å½±å“: {priority['impact']}
  - å·¥ä½œé‡: {priority['effort']}

"""
        
        # é£é™©ç‚¹
        report += f"""## 4. é£é™©ç‚¹è¯†åˆ«

"""
        for risk in risks:
            report += f"""### {risk['type']}
- **æè¿°**: {risk['description']}
- **ä¸¥é‡ç¨‹åº¦**: {risk['severity']}
- **ç¼“è§£æªæ–½**: {risk['mitigation']}

"""
        
        # å»ºè®®
        report += f"""## 5. é‡æ„å»ºè®®

### 5.1 çŸ­æœŸç›®æ ‡ï¼ˆ1-2å‘¨ï¼‰
1. è§£å†³é«˜ä¼˜å…ˆçº§æ¶æ„é—®é¢˜
2. é‡æ„é«˜å¤æ‚åº¦æ¨¡å—
3. å»ºç«‹åŸºç¡€DDDåˆ†å±‚ç»“æ„

### 5.2 ä¸­æœŸç›®æ ‡ï¼ˆ1ä¸ªæœˆï¼‰
1. å®Œå–„ä¾èµ–æ³¨å…¥æœºåˆ¶
2. ä¼˜åŒ–æ¨¡å—é—´ä¾èµ–å…³ç³»
3. å»ºç«‹ä»£ç è§„èŒƒ

### 5.3 é•¿æœŸç›®æ ‡ï¼ˆ2-3ä¸ªæœˆï¼‰
1. å®ç°å®Œæ•´çš„DDDæ¶æ„
2. å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•ä½“ç³»
3. ä¼˜åŒ–æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§

## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. æ ¹æ®ä¼˜å…ˆçº§åˆ¶å®šè¯¦ç»†çš„é‡æ„è®¡åˆ’
2. å»ºç«‹é‡æ„é‡Œç¨‹ç¢‘å’Œæ£€æŸ¥ç‚¹
3. å‡†å¤‡ç¬¬ä¸‰é˜¶æ®µï¼šç»“æ„è¿ç§»
"""
        
        return report

    def print_summary(self, patterns: List[Dict], dependencies: Dict, priorities: List[Dict], risks: List[Dict]) -> None:
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šæˆ˜åœºè§„åˆ’åˆ†æå®Œæˆï¼")
        print("="*60)
        
        print("\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        
        # æ¶æ„æ¨¡å¼
        print(f"  â€¢ æ¶æ„æ¨¡å¼åˆ†æ: {len(patterns)} ä¸ªæ¨¡å¼")
        for pattern in patterns:
            print(f"    - {pattern['pattern']}: {pattern['compliance']:.1%} åˆè§„ç‡")
        
        # ä¾èµ–å…³ç³»
        if 'metrics' in dependencies:
            metrics = dependencies['metrics']
            print(f"  â€¢ ä¾èµ–å…³ç³»: {metrics['total_modules']} ä¸ªæ¨¡å—ï¼Œå¹³å‡ {metrics['avg_dependencies']:.1f} ä¸ªä¾èµ–")
        
        # é‡æ„ä¼˜å…ˆçº§
        high_priority = [p for p in priorities if p['priority'] == 'high']
        print(f"  â€¢ é‡æ„ä¼˜å…ˆçº§: {len(high_priority)} ä¸ªé«˜ä¼˜å…ˆçº§æ¨¡å—")
        
        # é£é™©ç‚¹
        high_risks = [r for r in risks if r['severity'] == 'high']
        print(f"  â€¢ é£é™©ç‚¹: {len(high_risks)} ä¸ªé«˜é£é™©ç‚¹")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {self.output_dir}/phase2_planning_report.md")
        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = ArchitectureAnalyzer(project_root)
        
        # ç”Ÿæˆè§„åˆ’æŠ¥å‘Š
        report_file = analyzer.generate_planning_report()
        
        # æ‰§è¡Œåˆ†æå¹¶æ‰“å°æ‘˜è¦
        patterns = analyzer.analyze_architecture_patterns()
        dependencies = analyzer.analyze_dependencies()
        priorities = analyzer.determine_refactor_priorities(patterns, dependencies)
        risks = analyzer.identify_risk_points(patterns, dependencies)
        analyzer.print_summary(patterns, dependencies, priorities, risks)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 