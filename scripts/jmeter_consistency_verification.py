#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
JMETERå‘½ä»¤è¡Œä¸å›¾å½¢ç•Œé¢ä¸€è‡´æ€§éªŒè¯è„šæœ¬
- å¯¹æ¯”ç›¸åŒé…ç½®ä¸‹å‘½ä»¤è¡Œå’Œå›¾å½¢ç•Œé¢çš„æµ‹è¯•ç»“æœ
- éªŒè¯å“åº”æ—¶é—´ã€æˆåŠŸç‡ç­‰å…³é”®æŒ‡æ ‡çš„ä¸€è‡´æ€§
- ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""
import os
import sys
import subprocess
import time
import json
import csv
import shutil
from datetime import datetime
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.insert(0, project_root)

from src.application.services.device_query_service import DeviceQueryService

class JMeterConsistencyVerifier:
    """JMETERä¸€è‡´æ€§éªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.jmeter_bin = os.path.join(project_root, "src", "tools", "jmeter", "bin", "jmeter.bat")
        self.jmx_file = os.path.join(project_root, "src", "tools", "jmeter", "bin", "register_test_python_format.jmx")
        self.csv_file = os.path.join(project_root, "src", "tools", "jmeter", "bin", "new_devices_100000.csv")
        self.results_dir = os.path.join(project_root, "src", "tools", "jmeter", "results")
        
        # æ•°æ®åº“é…ç½®
        self.db_config = {
            'host': '192.168.24.45',
            'port': 3307,
            'user': 'root',
            'password': 'At6mj*1ygb2',
            'database': 'yangguan'
        }
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.results_dir, exist_ok=True)
        
    def modify_jmx_threads(self, thread_count, loops=1):
        """ä¿®æ”¹JMXæ–‡ä»¶ä¸­çš„çº¿ç¨‹æ•°å’Œå¾ªç¯æ¬¡æ•°"""
        print(f"ğŸ”§ ä¿®æ”¹JMXé…ç½®: çº¿ç¨‹æ•°={thread_count}, å¾ªç¯æ¬¡æ•°={loops}")
        
        # è¯»å–JMXæ–‡ä»¶
        with open(self.jmx_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä¿®æ”¹çº¿ç¨‹æ•°
        import re
        content = re.sub(
            r'<intProp name="ThreadGroup\.num_threads">\d+</intProp>',
            f'<intProp name="ThreadGroup.num_threads">{thread_count}</intProp>',
            content
        )
        
        # ä¿®æ”¹å¾ªç¯æ¬¡æ•°
        content = re.sub(
            r'<stringProp name="LoopController\.loops">\d+</stringProp>',
            f'<stringProp name="LoopController.loops">{loops}</stringProp>',
            content
        )
        
        # ä¿å­˜ä¿®æ”¹åçš„JMXæ–‡ä»¶
        modified_jmx = os.path.join(self.results_dir, f"consistency_test_{thread_count}threads_{loops}loops.jmx")
        with open(modified_jmx, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ ä¿®æ”¹åçš„JMXæ–‡ä»¶å·²ä¿å­˜: {modified_jmx}")
        return modified_jmx
    
    def run_command_line_test(self, jmx_path, test_name):
        """è¿è¡Œå‘½ä»¤è¡Œæµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹å‘½ä»¤è¡Œæµ‹è¯•: {test_name}")
        
        # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        jtl_file = os.path.join(self.results_dir, f"cli_{test_name}_{timestamp}.jtl")
        log_file = os.path.join(self.results_dir, f"cli_{test_name}_{timestamp}.log")
        
        # æ„å»ºJMETERå‘½ä»¤
        cmd = [
            self.jmeter_bin,
            "-n",  # éGUIæ¨¡å¼
            "-t", jmx_path,  # æµ‹è¯•è®¡åˆ’æ–‡ä»¶
            "-l", jtl_file,  # ç»“æœæ–‡ä»¶
            "-j", log_file,  # æ—¥å¿—æ–‡ä»¶
        ]
        
        print(f"ğŸ“‹ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        try:
            # æ‰§è¡ŒJMETERå‘½ä»¤
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                cwd=os.path.dirname(self.jmeter_bin)
            )
            
            # å®æ—¶è¾“å‡ºè¿›åº¦
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(f"   {output.strip()}")
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if return_code == 0:
                print(f"âœ… å‘½ä»¤è¡Œæµ‹è¯•å®Œæˆ: {test_name}")
                print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
                return True, jtl_file, duration
            else:
                error_output = process.stderr.read()
                print(f"âŒ å‘½ä»¤è¡Œæµ‹è¯•å¤±è´¥: {test_name}")
                print(f"é”™è¯¯ä¿¡æ¯: {error_output}")
                return False, None, duration
                
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå‘½ä»¤è¡Œæµ‹è¯•å¤±è´¥: {e}")
            return False, None, 0
    
    def run_gui_test(self, jmx_path, test_name):
        """è¿è¡Œå›¾å½¢ç•Œé¢æµ‹è¯•ï¼ˆé€šè¿‡GUIæ¨¡å¼ï¼‰"""
        print(f"ğŸ–¥ï¸  å¼€å§‹å›¾å½¢ç•Œé¢æµ‹è¯•: {test_name}")
        
        # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        jtl_file = os.path.join(self.results_dir, f"gui_{test_name}_{timestamp}.jtl")
        log_file = os.path.join(self.results_dir, f"gui_{test_name}_{timestamp}.log")
        
        # æ„å»ºJMETER GUIå‘½ä»¤
        cmd = [
            self.jmeter_bin,
            "-t", jmx_path,  # æµ‹è¯•è®¡åˆ’æ–‡ä»¶
            "-l", jtl_file,  # ç»“æœæ–‡ä»¶
            "-j", log_file,  # æ—¥å¿—æ–‡ä»¶
        ]
        
        print(f"ğŸ“‹ æ‰§è¡ŒGUIå‘½ä»¤: {' '.join(cmd)}")
        print("âš ï¸  æ³¨æ„ï¼šGUIæ¨¡å¼éœ€è¦æ‰‹åŠ¨å¯åŠ¨å’Œåœæ­¢æµ‹è¯•")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        try:
            # å¯åŠ¨GUIæ¨¡å¼
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                cwd=os.path.dirname(self.jmeter_bin)
            )
            
            print("ğŸ–¥ï¸  JMETER GUIå·²å¯åŠ¨ï¼Œè¯·åœ¨GUIä¸­æ‰‹åŠ¨å¯åŠ¨æµ‹è¯•...")
            print("â³ ç­‰å¾…æµ‹è¯•å®Œæˆ...")
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®Œæˆæµ‹è¯•
            input("è¯·æŒ‰å›è½¦é”®ç¡®è®¤æµ‹è¯•å·²å®Œæˆ...")
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # æ£€æŸ¥ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(jtl_file):
                print(f"âœ… å›¾å½¢ç•Œé¢æµ‹è¯•å®Œæˆ: {test_name}")
                print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
                return True, jtl_file, duration
            else:
                print(f"âŒ å›¾å½¢ç•Œé¢æµ‹è¯•å¤±è´¥: æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
                return False, None, duration
                
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå›¾å½¢ç•Œé¢æµ‹è¯•å¤±è´¥: {e}")
            return False, None, 0
    
    def analyze_jtl_file(self, jtl_file, test_type):
        """åˆ†æJTLæ–‡ä»¶ï¼Œç»Ÿè®¡æµ‹è¯•ç»“æœ"""
        print(f"ğŸ“Š åˆ†æ{test_type}æµ‹è¯•ç»“æœ: {jtl_file}")
        
        if not os.path.exists(jtl_file):
            print(f"âŒ JTLæ–‡ä»¶ä¸å­˜åœ¨: {jtl_file}")
            return {}
        
        stats = {
            'test_type': test_type,
            'total_requests': 0,
            'success_count': 0,
            'fail_count': 0,
            'avg_response_time': 0,
            'min_response_time': float('inf'),
            'max_response_time': 0,
            'response_times': [],
            'throughput': 0,
            'error_rate': 0
        }
        
        try:
            with open(jtl_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    stats['total_requests'] += 1
                    
                    # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
                    if row.get('success', '').lower() == 'true':
                        stats['success_count'] += 1
                    else:
                        stats['fail_count'] += 1
                    
                    # ç»Ÿè®¡å“åº”æ—¶é—´
                    try:
                        response_time = float(row.get('elapsed', 0))
                        stats['response_times'].append(response_time)
                        stats['min_response_time'] = min(stats['min_response_time'], response_time)
                        stats['max_response_time'] = max(stats['max_response_time'], response_time)
                    except (ValueError, TypeError):
                        pass
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if stats['response_times']:
                stats['avg_response_time'] = sum(stats['response_times']) / len(stats['response_times'])
                stats['min_response_time'] = min(stats['response_times'])
                stats['max_response_time'] = max(stats['response_times'])
            
            # è®¡ç®—æˆåŠŸç‡å’Œé”™è¯¯ç‡
            if stats['total_requests'] > 0:
                stats['success_rate'] = stats['success_count'] / stats['total_requests'] * 100
                stats['error_rate'] = stats['fail_count'] / stats['total_requests'] * 100
            
            print(f"ğŸ“ˆ {test_type}æµ‹è¯•ç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
            print(f"   æˆåŠŸæ•°: {stats['success_count']}")
            print(f"   å¤±è´¥æ•°: {stats['fail_count']}")
            print(f"   æˆåŠŸç‡: {stats.get('success_rate', 0):.2f}%")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ms")
            print(f"   æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.2f}ms")
            print(f"   æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.2f}ms")
            
            return stats
            
        except Exception as e:
            print(f"âŒ åˆ†æJTLæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def compare_results(self, cli_stats, gui_stats):
        """å¯¹æ¯”å‘½ä»¤è¡Œå’Œå›¾å½¢ç•Œé¢çš„æµ‹è¯•ç»“æœ"""
        print("\nğŸ” å¯¹æ¯”åˆ†æç»“æœ")
        print("="*60)
        
        comparison = {
            'metrics': {},
            'differences': {},
            'consistency_score': 0
        }
        
        # å¯¹æ¯”å…³é”®æŒ‡æ ‡
        metrics_to_compare = [
            'total_requests', 'success_count', 'fail_count', 
            'avg_response_time', 'min_response_time', 'max_response_time',
            'success_rate', 'error_rate'
        ]
        
        total_differences = 0
        total_metrics = len(metrics_to_compare)
        
        for metric in metrics_to_compare:
            cli_value = cli_stats.get(metric, 0)
            gui_value = gui_stats.get(metric, 0)
            
            comparison['metrics'][metric] = {
                'cli': cli_value,
                'gui': gui_value,
                'difference': abs(cli_value - gui_value),
                'percentage_diff': abs(cli_value - gui_value) / max(cli_value, 1) * 100
            }
            
            # è®¡ç®—å·®å¼‚
            if metric in ['avg_response_time', 'min_response_time', 'max_response_time']:
                # å“åº”æ—¶é—´å…è®¸5%çš„å·®å¼‚
                threshold = 5.0
            else:
                # å…¶ä»–æŒ‡æ ‡å…è®¸1%çš„å·®å¼‚
                threshold = 1.0
            
            if comparison['metrics'][metric]['percentage_diff'] > threshold:
                total_differences += 1
                comparison['differences'][metric] = {
                    'cli_value': cli_value,
                    'gui_value': gui_value,
                    'difference': abs(cli_value - gui_value),
                    'percentage_diff': comparison['metrics'][metric]['percentage_diff']
                }
        
        # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
        consistency_score = ((total_metrics - total_differences) / total_metrics) * 100
        comparison['consistency_score'] = consistency_score
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print(f"ğŸ“Š ä¸€è‡´æ€§å¾—åˆ†: {consistency_score:.2f}%")
        print(f"ğŸ“ˆ å·®å¼‚æŒ‡æ ‡æ•°é‡: {total_differences}/{total_metrics}")
        
        if total_differences == 0:
            print("âœ… æ‰€æœ‰æŒ‡æ ‡å®Œå…¨ä¸€è‡´ï¼")
        else:
            print("âš ï¸  å‘ç°å·®å¼‚æŒ‡æ ‡:")
            for metric, diff_info in comparison['differences'].items():
                print(f"   {metric}: CLI={diff_info['cli_value']:.2f}, GUI={diff_info['gui_value']:.2f}, å·®å¼‚={diff_info['percentage_diff']:.2f}%")
        
        return comparison
    
    def generate_comparison_report(self, cli_stats, gui_stats, comparison, test_config):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        csv_file = os.path.join(self.results_dir, f"consistency_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = [
                'æµ‹è¯•æŒ‡æ ‡', 'å‘½ä»¤è¡Œç»“æœ', 'å›¾å½¢ç•Œé¢ç»“æœ', 'ç»å¯¹å·®å¼‚', 'ç™¾åˆ†æ¯”å·®å¼‚(%)', 'æ˜¯å¦ä¸€è‡´'
            ]
            
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for metric, comp_data in comparison['metrics'].items():
                is_consistent = metric not in comparison['differences']
                writer.writerow({
                    'æµ‹è¯•æŒ‡æ ‡': metric,
                    'å‘½ä»¤è¡Œç»“æœ': f"{comp_data['cli']:.2f}",
                    'å›¾å½¢ç•Œé¢ç»“æœ': f"{comp_data['gui']:.2f}",
                    'ç»å¯¹å·®å¼‚': f"{comp_data['difference']:.2f}",
                    'ç™¾åˆ†æ¯”å·®å¼‚(%)': f"{comp_data['percentage_diff']:.2f}",
                    'æ˜¯å¦ä¸€è‡´': 'æ˜¯' if is_consistent else 'å¦'
                })
            
            # æ·»åŠ æ€»ç»“è¡Œ
            writer.writerow({
                'æµ‹è¯•æŒ‡æ ‡': 'ä¸€è‡´æ€§å¾—åˆ†',
                'å‘½ä»¤è¡Œç»“æœ': '',
                'å›¾å½¢ç•Œé¢ç»“æœ': '',
                'ç»å¯¹å·®å¼‚': '',
                'ç™¾åˆ†æ¯”å·®å¼‚(%)': f"{comparison['consistency_score']:.2f}%",
                'æ˜¯å¦ä¸€è‡´': 'é€šè¿‡' if comparison['consistency_score'] >= 95 else 'éœ€è¦å…³æ³¨'
            })
        
        print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {csv_file}")
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        json_file = os.path.join(self.results_dir, f"consistency_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        report_data = {
            'test_config': test_config,
            'cli_stats': cli_stats,
            'gui_stats': gui_stats,
            'comparison': comparison,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {json_file}")
        
        return csv_file, json_file
    
    def run_consistency_test(self, thread_count, loops=1):
        """è¿è¡Œä¸€è‡´æ€§æµ‹è¯•"""
        print(f"ğŸ¯ å¼€å§‹ä¸€è‡´æ€§éªŒè¯æµ‹è¯•")
        print("="*60)
        print(f"ğŸ§µ çº¿ç¨‹æ•°: {thread_count}")
        print(f"ğŸ”„ å¾ªç¯æ¬¡æ•°: {loops}")
        print("="*60)
        
        # ä¿®æ”¹JMXæ–‡ä»¶
        modified_jmx = self.modify_jmx_threads(thread_count, loops)
        test_name = f"{thread_count}threads_{loops}loops"
        
        # è¿è¡Œå‘½ä»¤è¡Œæµ‹è¯•
        cli_success, cli_jtl, cli_duration = self.run_command_line_test(modified_jmx, test_name)
        
        if not cli_success:
            print("âŒ å‘½ä»¤è¡Œæµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
            return False
        
        # åˆ†æå‘½ä»¤è¡Œç»“æœ
        cli_stats = self.analyze_jtl_file(cli_jtl, "å‘½ä»¤è¡Œ")
        
        print(f"\nâ³ ç­‰å¾…5ç§’åå¼€å§‹å›¾å½¢ç•Œé¢æµ‹è¯•...")
        time.sleep(5)
        
        # è¿è¡Œå›¾å½¢ç•Œé¢æµ‹è¯•
        gui_success, gui_jtl, gui_duration = self.run_gui_test(modified_jmx, test_name)
        
        if not gui_success:
            print("âŒ å›¾å½¢ç•Œé¢æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
            return False
        
        # åˆ†æå›¾å½¢ç•Œé¢ç»“æœ
        gui_stats = self.analyze_jtl_file(gui_jtl, "å›¾å½¢ç•Œé¢")
        
        # å¯¹æ¯”ç»“æœ
        comparison = self.compare_results(cli_stats, gui_stats)
        
        # ç”ŸæˆæŠ¥å‘Š
        test_config = {
            'thread_count': thread_count,
            'loops': loops,
            'cli_duration': cli_duration,
            'gui_duration': gui_duration
        }
        
        self.generate_comparison_report(cli_stats, gui_stats, comparison, test_config)
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='JMETERå‘½ä»¤è¡Œä¸å›¾å½¢ç•Œé¢ä¸€è‡´æ€§éªŒè¯è„šæœ¬')
    parser.add_argument('--threads', type=int, default=10,
                       help='çº¿ç¨‹æ•°ï¼Œä¾‹å¦‚: 10')
    parser.add_argument('--loops', type=int, default=1,
                       help='å¾ªç¯æ¬¡æ•°ï¼Œä¾‹å¦‚: 1')
    
    args = parser.parse_args()
    
    print("ğŸ¯ JMETERä¸€è‡´æ€§éªŒè¯è„šæœ¬")
    print("="*60)
    print(f"ğŸ“‹ æµ‹è¯•é…ç½®: {args.threads}çº¿ç¨‹ Ã— {args.loops}å¾ªç¯")
    print("="*60)
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶æ‰§è¡Œæµ‹è¯•
    verifier = JMeterConsistencyVerifier()
    success = verifier.run_consistency_test(args.threads, args.loops)
    
    if success:
        print("\nğŸ‰ ä¸€è‡´æ€§éªŒè¯å®Œæˆï¼")
    else:
        print("\nâŒ ä¸€è‡´æ€§éªŒè¯å¤±è´¥ï¼")

if __name__ == '__main__':
    main() 